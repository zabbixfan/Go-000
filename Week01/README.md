学习笔记
微服务概览
单体架构
Q: 由于是单体式应用，打包时过于庞大。应用无法扩展，可靠性低。无法完成敏捷开发与部署。
A: 应用内部模块化，分而治之。

微服务起源
Q: 微服务与 SOA(Service-Oriented Architecture)的关系
A: 微服务是 SOA 的一种实践
Q: 微服务要点
A:

小既是美。
单一职责。
从单 db 到多 db。
尽可能早的创建原型(即先与前端约定接口服务,保证服务间沟通的一致性)。
可移植性比效率更重要(比如从多协议到单一协议，gRPC)。
微服务定义
原子服务：按功能划分只关注单一业务、闭环业务。颗粒度粗属于初级划分
独立进程：虚拟化技术，每个服务独立虚拟环境部署。
隔离部署：由于 Docker 的发展推动了微服务的发展。通过对每个服务的隔离部署，不会出现一个服务挂掉导致所有服务挂掉的问题。
去中心化服务治理：后面会提
基础设施的建设、复杂度高：这是问题，解决方案：利用大量的中间件与微服务基础设施弥补
微服务不足
对于消息的传递与通信要求高。
Q1: 可能因为烂代码(如 for 循环查询)造成上游到下游的网络请求成倍大增。A1: batch 化，提供粗粒度的批量接口。
Q2: 串行化造成消息传递过慢。A2: 改串行为并行
由于微服务的多 db 特性，在 db 的一致性与事务的处理上是一个更高难度的挑战。
测试微服务应用成为难题。
服务间的模块会形成依赖，当对某个服务进行升级修改时，可能造成多个服务模块的修改。
对运维基础设施的挑战难度加大。Q: 多机器问题无法排查。A: 日志统一采集，指标汇总，监控体系升级，报警系统升级，用上 k8s。
组件服务化
将单体应用的多组件拆分为多服务化每个服务化单一组件。好处是：单一服务的变更不会影响到全部的服务
Q: 如何利用 go 实施一个微服务？ A: 1、kit: 微服务基础库(如 go-kit、go-mirco) 2、service: 业务代码+kit 依赖+第三方依赖组成的业务微服务 3、rpc+message queue: 利用 rpc 协议(gRPC)和消息队列(kafka)实现轻量级通信
组件服务化的本质：多个微服务组合构成的完整用户场景(即可交付的可给用户使用的项目)
按业务组织服务
团队结构：
烟囱式(垂直划分): 开发与运维只负责职位职能范围内的事。即开发只开发不了解如何发布服务。
全栈：开发需要为整个业务场景负责，从测试到发布到监控都由开发一手包办，因为开发最了解自己的代码。测试与运维平台化、开发化，由测试与运维开发相应的测试平台、发布平台、日志采集平台、监控平台等。
团队模式
大前端 <=> 网关: 大前端与网关团队形成闭环。
网关 <=> 业务服务: 通过 API 对接
测试与运维: 提供好用的基础设施
开发团队对软件在生产环境的运行负全部责任: You build it, You fix it.
去中心化
数据去中心化 ：每个服务独占一个 DB、redis,通过服务间的通信来做沟通，体现出 MicroService 的隔离性
治理去中心化 ：很多场景有可能造成流量热点，对 nginx 造成大量压力，需要从 nginx 转发转变为直连。(并不是所有场景！！！)
技术去中心化 ：在技术上进行收敛(比如多语言收敛成少语言，多 rpc 协议收敛到统一 rpc 协议)
基础设施自动化
CICD ：Gitlab + Gitlab Hooks + k8s (编译、测试、打包、发布流程化)
Testing ：测试环境、单元测试、API 自动化测试(YApi)
在线运行时 ：k8s、Prometheus(指标采集,alertManage 报警)、ELK(日志采集)、Control Panel(微服务治理控制面板)
可用性 & 兼容性设计
以 Design For Failure 思想为核心，即鲁棒性编程
采用粗粒度进行进程间通信，比如接口 batch 化
可用性问题 ：隔离、超时、负载保护、限流、降级、重试、负载均衡...
必须保持接口兼容性
发送时保守，接受时开放 ：最小化传送必要的信息，最大化程度容忍冗余数据(通过数据校验保证兼容性)。
微服务设计
某大型交友网站 1.0
从单体结构按照垂直功能进行拆分服务，所有服务接口对外暴露。显示出大量问题：
C<->S 直接通信，强耦合。
C 端需要多次请求，在 C 端进行数据耦合，造成 C 端开发的高工作量、高延迟。(面向用户业务场景的 API,而不是面向资源的 API)。
协议不统一
面向 C 的 API 适配
多终端兼容逻辑复杂
统一逻辑无法收敛，如果 auth、限流，修复漏洞需要所有服务一起升级
需要将工作模式内聚化
某大型交友网站 2.0
app-interface/BFF(Backend For Frontend)
统一协议，以网关小组作为中间层，统一各个服务的协议发给 C 端。
可将 BFF 认为是一种适配服务，即聚合 S 端向 C 端暴露统一协议的 API。BFF 用作数据整合。
轻量交互 ：针对不同设备做协议适配
差异服务 ：针对不同终端定制化 API
动态升级 ：服务端可以随意升级,不会影响到 C 端与网关的交互
沟通效率提升
某大型交友网站 3.0
single point of failure
BFF 属于 single point of failure，可能由于代码缺陷或者流量洪峰引发的集体宕机。
Q1: 单个模块导致后续业务集成复杂度高，团队之间协调成本高，交付效率低下。
A1: 可将单一高集成度的 BFF 拆分为多个较低集成度 BFF。
Q2: 由于安全认证、日志监控、限流熔断等横切面逻辑都集成到每个 BFF 上，当 BFF 节点过多时，升级横切面逻辑造成多节点需要一起升级，过于繁琐复杂。
某大型交友网站 4.0
API GateWay
将跨横切面(Cross-Cutting Concerns)的功能,如路由、认证、限流、安全等全部从 BFF 中抽出并上沉到 GateWay(Envoy)。关注点分离。
最后的分层：服务层只暴露在内网中。BFF 用于业务组装，通过 API GateWay 暴露对外。
在本网站中 BFF 由 nodejs 做服务端渲染(SSR, Server-Side Rendering)
API GateWay 上层还有 CDN、4/7 层负载均衡(ELB)
MircoService 划分
按照部门职能划分
DDD 限界上下文划分，面向业务场景划分(推荐！闭环！)
从组织结构上 形成闭环的部门划分为一个服务，而不是太过分散
服务过细需整合
CQRS(Command Query Responsibility Segregation)
读写分离
从 pull 变成 push 即从查询变成推送，订阅化。(db -> canal -> MQ 中间件)
MircoService 安全
API Gateway 基于业务的 auth 鉴权 Token -> JWT
BFF 校验 Token 完整性并将身份信息 JWT -> UserID
API GateWay -> BFF -> Service 《==》 Biz Auth -> JWT -> Request Args
Full Trust, Half Trust, Zero Trust
gRPC & 服务发现
gRPC (具体可以http://www.google.com)
多语言：语言中立
轻量级、高性能：PB，代码即文档 文档即代码
可插拔：支持插件
IDL：可通过 proto 生成代码
设计理念
移动端：支持标准 http2、支持 TCP 单链接复用。(这里建议可看 net/rpc 的源码)
服务而非对象、消息非引用
负载无关的：可使用不同的消息类型和编码 pb、JSON、XML、Thrift
流
阻塞式和非阻塞式
元数据交换
标准状态码
gRPC - HealthCheck(健康检查)
主动健康检测：在消费者从服务发现中获取服务是向服务发送健康检测，如果检测结果不好就换一个，减少错误请求。
外挂注册：服务不主动向服务发现注册服务，而是通过一个外部服务对该服务进行健康检测，如果该服务健康向服务发现注册。
平滑发布/下线：
kill pid
Service -> Discovery 注销
Discovery -> all Consumers 注销
health check 注销
等待两个心跳周期，用来处理所有当前正在处理的请求
如果还不退出就 kill -9
服务发现
客户端发现
服务向注册中心注册服务，C 端定时从注册中心拉取当前所有服务。在 C 端中内置 LB，C 端可直连服务端，实现去治理中心化，数据传输更高效。
服务端发现
C 端通过请求负载均衡器由负载均衡器做负载均衡与转发，负载均衡器在每次转发时都从注册中心查询服务通过 LB 算法做转发。开发人员不必自己实现 LB 算法，节约开发成本。
需要多节点+LVS 等等一系列东西 过于繁琐
客户端发现 vs 服务端发现
客户端发现：直连，少一次网络跳转。但 Consumer 需要内置特定的服务发现客户端和发现逻辑、还有 LB 算法
服务端发现：consumer 无需关注服务发现的具体细节只需要知道服务的 DNS 域名，支持多语言开发，但需要基础设施支撑，多了一次网络跳转会有性能损耗。
总结：微服务的特点就是去中心化，所以使用客户端发现模式。
service mash(不建议)：资源开销大，多语言可共用一种/一套 LB，app 与 LB 整合在一个 pod 中，通过 rpc 连接，更高效。
服务发现选型
Consul zookeeper etcd eureka
ca cp cp ap
代码一坨屎 cp 理论中会有 leader,在平滑下线时会形成广播风暴,引起雪崩,扛不住海量服务 同 ⬅️ ap 弱一致性,但 2.0 咕咕咕了,推荐根据 eureka 实现的 bilibili-discovery
eureka
注册时延迟：新建的节点慢几秒接受到流量并不会有多大的影响
注销时延迟：因为有 health Check，在服务注销时可以从 consumer 发送 health Check 检测服务是否可用
如果想直接使用的话,推荐 Nacos
bilibili-discovery 实现原理(同步课件)
注册时发送 Appid 和 IP:Port 及一些元数据：权重、染色、集群等。 appid：三段式命名,business.service.xxx。
服务每 30s 心跳一次，保证自己存活，下线也需要发送下线通知。
Consumer 向服务发现发起 30s 长轮询更新获取服务。
服务发现定期(60s)检测失效(90s)的实例。若短时间内丢失大量心跳连接(15 分钟心跳低于期望值的 85%)，则开启自我保护，保留过期服务。
consumer - Discovery 连接暂时中断：consumer 可以从上次在 Discovery 中拉取的连接继续使用，短暂失效问题 8 大
consumer - provide 连接中断,可在 health Check 检测出来
Discovery 单节点挂掉,不立即重启,快速恢复。全部挂掉，得等所有服务注册后才提供 Discovery 服务。单节点挂掉后，新增节点， 新节点接受所有旧节点的数据，若先接受新数据后接受旧数据，因为会有版本控制功能，所以不用担心会出现旧数据覆盖新数据的场景。
多集群 & 多租户
多集群
L0 服务 单集群->多业务多集群(每个业务对应一个集群，cache 多套冗余)
单集群多节点保证服务可用性
从单一集群故障考虑多集群
单个机房故障导致的问题
多业务多集群弊端(每个业务对应一个集群)
Q1: 当某个业务对应的集群挂掉时，连接转发到另一个集群，但另一个集群的热点缓存与当前业务的热点不相同。数据无正交性。
A1: 将 consumer 全连接所有的集群，这样可以将所有 cache 的内容正交化，数据热点为全业务热点。
Q2: ⬆️ 方法使得每个集群接受所有服务的 health Check 导致 cpu 高负荷，高达 30%。
A2: 使用子集算法(参考 main.go)，将后端节点均分给 consumer，客户端重启时重新保存均衡，对后端重启保持透明，连接变动最小
cache 清理：通过反向订阅 mysql 的 binlog 反向清理 cache
多租户
并行测试的问题
混用环境导致的不可靠测试
多套环境的硬件成本
难做压测，压测结果不可信
染色发布
基于流入栈中的流量类型做路由
具有隔离性
可自定义拓展
请求通过绑定 context 实现数据隔离的流量路由
